{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess import *\n",
    "from pdftotext import pdftotext\n",
    "from detect_encoding import detect_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_I(path):\n",
    "    output, filename = pdftotext(path)\n",
    "    fp = f\"{filename}.txt\"\n",
    "    type = detect_encoding(fp)\n",
    "    strip, n = preprocess_fun(fp, type)\n",
    "    input, n = writeTo(strip, n, type)\n",
    "    return remove_stopwords(n, type), type\n",
    "\n",
    "def preprocess_fun(file, enc):\n",
    "    if file.endswith('.txt'):\n",
    "        n = os.path.basename(file)\n",
    "        name = re.sub(r\"(\\.txt$|\\d)\", '', n)\n",
    "        with open(file, encoding=enc) as input:\n",
    "            raw = input.read()\n",
    "            lower = raw.lower()\n",
    "            rem_punc = re.sub(r\"[.!?,:;/'&*-_#$()]|[\\\"']\",'',lower)\n",
    "            strip = rem_punc.strip()\n",
    "      \n",
    "        return strip, name\n",
    "\n",
    "def writeTo(text_content, name, enc):\n",
    "    with open(f'{name}(processed).txt','w',encoding=enc) as input:\n",
    "        input.write(text_content)\n",
    "        return input, name\n",
    "\n",
    "def remove_stopwords(name, enc):\n",
    "    with open(f\"{name}(processed).txt\", encoding=enc) as input:\n",
    "        raw = input.read()\n",
    "        doc = nlp(raw)\n",
    "        stop_removed = \" \".join([word.text for word in doc if not word.is_stop])\n",
    "        # sparse = [word for word in stop_removed if stop_removed.count(word) < 10]  \n",
    "\n",
    "    with open(f'{name}(spacy).txt', 'w', encoding=enc) as  output:\n",
    "        output.write(stop_removed)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def preprocess_I(path):\n",
    "    output, filename = pdftotext(path)\n",
    "    fp = f\"{filename}.txt\"\n",
    "    type = detect_encoding(fp)\n",
    "    strip, n = preprocess_fun(fp, type)\n",
    "    input, n = writeTo(strip, n, type)\n",
    "    \n",
    "    \n",
    "    return remove_stopwords(n, type), type\n",
    "\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(filepath):\n",
    "    with open(filepath, 'rb') as input:\n",
    "        D = chardet.universaldetector.UniversalDetector()\n",
    "        for i in input:\n",
    "            D.feed(i)\n",
    "            if D.done:\n",
    "                break\n",
    "        D.close()\n",
    "        print(D.result)\n",
    "        return D.result['encoding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(chunked_words, df, plot):\n",
    "    rejoined = ' '.join(chunked_words)\n",
    "    resplit = rejoined.split()\n",
    "    per_word_freq = pd.Series(resplit).value_counts()\n",
    "\n",
    "    wp_mle = per_word_freq / len(resplit)\n",
    "    wp_mle_sorted = wp_mle.sort_values(ascending=False)\n",
    "\n",
    "    top_words_by_mle = wp_mle_sorted.head(10).index\n",
    "    \n",
    "    mle_freq_df = pd.DataFrame(0, index=np.arange(len(df)), columns=top_words_by_mle)\n",
    "    display(mle_freq_df)\n",
    "\n",
    "    for i, tengram in enumerate(df['10-chunk']):\n",
    "        tengram_split = tengram.split()\n",
    "        for word in tengram_split:\n",
    "            if word in top_words_by_mle:\n",
    "                mle_freq_df.loc[i, word] += 1\n",
    "\n",
    "    \n",
    "    plot[1].set_title('Top 10 [MLE]')\n",
    "    for i in mle_freq_df.dropna():\n",
    "        plot[1].plot(mle_freq_df, label=i)\n",
    "\n",
    "    plot[1].set_ylabel('Frequency')\n",
    "    plot[1].set_xlabel('Chunks | 10-gram')\n",
    "    plot[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file, enc = preprocess_I(\"/Users/andreas/Desktop/DCLH_Research/disregard/comte/presentation_of_self.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/andreas/Desktop/DCLH_Research/presentation_of_self.pdf(spacy).txt\", 'r', encoding=enc) as rf:\n",
    "    file = rf.read()\n",
    "    ngram = file.split()\n",
    "    word_groups = [' '.join(ngram[word:word+10]) for word in range(0, len(ngram), 10)]\n",
    "    word_group_df = pd.DataFrame(word_groups, columns=['10-chunk'])\n",
    "\n",
    "print(f\"WG Head: \\n {word_group_df.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ()\n",
    "mle(word_groups, word_group_df, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
