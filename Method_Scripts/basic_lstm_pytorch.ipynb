{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["training = [(\"The bear mauled the coyote\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n","            (\"Smokers despise themselves some would say, not me though\".split(), [\"NN\", \"V\", \"DET\", \"DET\", \"V\", \"V\", \"DET\", \"NN\", \"DET\"])]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def word_wise_indexing(text, text_index):\n","    sequence = text\n","    element_index = [text_index[word_element] for word_element in sequence]\n","    return torch.tensor(element_index, dtype=torch.long)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["training_data_index = {}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'The': 0, 'bear': 1, 'mauled': 2, 'the': 3, 'coyote': 4, 'Smokers': 5, 'despise': 6, 'themselves': 7, 'some': 8, 'would': 9, 'say,': 10, 'not': 11, 'me': 12, 'though': 13}\n"]}],"source":["for sentence, tags in training:\n","    for word in sentence:\n","        if word not in training_data_index:\n","            training_data_index[word] = len(training_data_index)\n","\n","print(training_data_index)\n","\n","tag_index = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n","\n","embedding_dims = 6\n","hi_hidden_dims = 6"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, emb_dim, hi_dim, vocab_size, tagset_size):\n","        super(LSTMTagger, self).__init__()\n","        self.hi_dim = hi_dim\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, emb_dim)\n","\n","        self.lstm_model = nn.LSTM(emb_dim, hi_dim)\n","\n","        self.transform_into_tag_space = nn.Linear(hi_dim, tagset_size)\n","    \n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        \n","        # The view() function below reshapes the tensor to satisfy the dimension perameters of the lstm function (sentence length, batch size, embedding dimensions).\n","        model_output, _ = self.lstm_model(embeds.view(len(sentence), 1, -1)) \n","\n","        model_output = model_output.view(len(sentence), self.hi_dim)\n","        \n","        # The view() function is used again prior to calling the 2D linear transformation function designed to map the LSTM model's hidden state to tag space.\n","        tag_space = self.transform_into_tag_space(model_output) # Returns a 2D tensor\n","\n","        # The log_softmax() function converts the model output score into log-probabilities for all elements in the tag space/original sequence (dim=1).\n","        scores = F.log_softmax(tag_space, dim=1)\n","\n","        return scores\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["lstm_model_run_one = LSTMTagger(embedding_dims, hi_hidden_dims, len(training_data_index), len(tag_index))\n","fn_loss = nn.NLLLoss()\n","fn_optimization = optim.SGD(lstm_model_run_one.parameters(), lr=0.1)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.9660, -0.9325, -1.4880],\n","        [-0.8993, -0.9933, -1.5017],\n","        [-0.9367, -1.0355, -1.3744],\n","        [-0.9673, -0.9975, -1.3819],\n","        [-0.9393, -0.9643, -1.4790]])\n"]}],"source":["with torch.no_grad():\n","    input_data = word_wise_indexing(training[0][0], training_data_index)\n","    scores_for_tags = lstm_model_run_one(input_data)\n","    \n","    print(scores_for_tags)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["for epoch in range(300):\n","    for sentence_sequence, assoc_tags in training:\n","        \n","        lstm_model_run_one.zero_grad()\n","\n","        input_segment = word_wise_indexing(sentence_sequence, training_data_index)\n","\n","        target_entities = word_wise_indexing(assoc_tags, tag_index)\n","\n","        seq_sen_score = lstm_model_run_one(input_segment)\n","\n","        with_fixed_loss_fn = fn_loss(seq_sen_score, target_entities)\n","        with_fixed_loss_fn.backward()\n","        fn_optimization.step()\n","    \n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.0558, -3.6985, -3.5225],\n","        [-5.6297, -0.0340, -3.5130],\n","        [-2.4759, -3.9727, -0.1086],\n","        [-0.0705, -3.5977, -3.2021],\n","        [-5.9280, -0.0226, -3.9258]])\n"]}],"source":["with torch.no_grad():\n","    input_data_post_training = word_wise_indexing(training[0][0], training_data_index)\n","\n","    post_training_scores = lstm_model_run_one(input_data_post_training)\n","\n","    print(post_training_scores)"]}],"metadata":{"kernelspec":{"display_name":"myenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
